1
00:00:00,329 --> 00:00:05,700
Якщо технічні ідеї, на яких базується

2
00:00:03,060 --> 00:00:07,470
глибинне навчання (ГН) і нейронні мережі (НМ),

3
00:00:05,700 --> 00:00:09,870
витають в повітрі ще з минулого століття, то чому ж

4
00:00:07,470 --> 00:00:12,090
вони розвинулись лише зараз? В цьому відео

5
00:00:09,870 --> 00:00:14,130
давай розглянемо головні рушійні сили

6
00:00:12,090 --> 00:00:16,170
розквіту ГН, бо

7
00:00:14,130 --> 00:00:18,090
я думаю, що це допоможе

8
00:00:16,170 --> 00:00:20,850
твоїй команді вибрати те,

9
00:00:18,090 --> 00:00:22,439
що краще застосувати. Протягом останніх

10
00:00:20,850 --> 00:00:24,240
кілька років багато людей

11
00:00:22,439 --> 00:00:26,820
питали мене: "Андрію, чому глибинне навчання

12
00:00:24,240 --> 00:00:28,949
так добре працює?". І коли

13
00:00:26,820 --> 00:00:31,109
я відповідаю на це питання, зазвичай,

14
00:00:28,949 --> 00:00:33,210
показую оце зображення. Скажімо,

15
00:00:31,109 --> 00:00:36,180
ми намалювали графік, де горизонтальна

16
00:00:33,210 --> 00:00:39,270
вісь відповідає за кількість даних, які нам дано

17
00:00:36,180 --> 00:00:42,570
для задачі, і, скажімо, вертикальна

18
00:00:39,270 --> 00:00:44,430
вісь відображає продуктивність

19
00:00:42,570 --> 00:00:48,180
навчальних алгоритмів, про які ми говорили раніше, 
наприклад, точність

20
00:00:44,430 --> 00:00:51,960
класифікатора спаму, або точність передбачення

21
00:00:48,180 --> 00:00:53,969
клацання по рекламі, або точність

22
00:00:51,960 --> 00:00:56,399
вказання НМ-ою позицій

23
00:00:53,969 --> 00:00:58,440
інших авто для безпілотного авто.

24
00:00:56,399 --> 00:01:00,270
Виявляється, якщо розглядати продуктивність

25
00:00:58,440 --> 00:01:02,460
традиційного навчального алгоритму, такого як

26
00:01:00,270 --> 00:01:04,710
метод опорних векторів (МОВ) або логістична

27
00:01:02,460 --> 00:01:07,619
регресія, як функцію від кількості

28
00:01:04,710 --> 00:01:09,720
наявних даних, то можна отримати

29
00:01:07,619 --> 00:01:11,670
приблизно ось таку криву, де

30
00:01:09,720 --> 00:01:14,280
продуктивність недовго збільшується, якщо

31
00:01:11,670 --> 00:01:16,200
збільшується кількість даних, але в певний момент

32
00:01:14,280 --> 00:01:18,630
переходить в плато.

33
00:01:16,200 --> 00:01:21,180
(Це мала б бути горизонтальна,

34
00:01:18,630 --> 00:01:25,320
проте, я не дуже добре її намалював).

35
00:01:21,180 --> 00:01:28,140
Вони не знають, що робити з великими

36
00:01:25,320 --> 00:01:30,689
об'ємами даних. В нашому

37
00:01:28,140 --> 00:01:32,850
суспільстві протягом останніх, можливо, 20 років

38
00:01:30,689 --> 00:01:34,820
для багатьох задач

39
00:01:32,850 --> 00:01:38,610
замість відносно малих об'ємів даних

40
00:01:34,820 --> 00:01:40,979
ми отримали часто досить великі

41
00:01:38,610 --> 00:01:43,979
об'єми даних. І багато з них -

42
00:01:40,979 --> 00:01:46,979
завдяки оцифруванню суспільства.

43
00:01:43,979 --> 00:01:48,720
Зараз дуже багато людської активності

44
00:01:46,979 --> 00:01:51,180
знаходиться в цифровому царстві. Ми проводимо дуже багато часу

45
00:01:48,720 --> 00:01:54,320
перед комп'ютерами, на сайтах, у мобільних

46
00:01:51,180 --> 00:01:57,960
додатках, і своєю активністю на цифрових пристроях

47
00:01:54,320 --> 00:02:00,360
створюємо дані. Завдяки поширенню

48
00:01:57,960 --> 00:02:02,369
недорогих камер, вбудованих в наші мобільні

49
00:02:00,360 --> 00:02:05,909
телефони, акселерометрам та іншим видам

50
00:02:02,369 --> 00:02:07,890
сенсорів в Інтернеті речей, ми

51
00:02:05,909 --> 00:02:11,129
також накопичуємо все більше

52
00:02:07,890 --> 00:02:12,870
і більше даних. Тож протягом останніх 20 років

53
00:02:11,129 --> 00:02:13,560
для багатьох задач ми

54
00:02:12,870 --> 00:02:16,319
накопичили

55
00:02:13,560 --> 00:02:17,550
об'єми даних більші, ніж традиційні

56
00:02:16,319 --> 00:02:20,520
навчальні алгоритми в змозі

57
00:02:17,550 --> 00:02:22,560
ефективно використовувати. А з допомогою НМ,

58
00:02:20,520 --> 00:02:26,310
виявляється, що у випадку

59
00:02:22,560 --> 00:02:28,470
тренування маленької НМ крива

60
00:02:26,310 --> 00:02:31,349
продуктивності буде приблизно ось такою.

61
00:02:28,470 --> 00:02:34,590
Якщо ми тренуємо трохи більшу НМ,

62
00:02:31,349 --> 00:02:36,330
назвемо її НМ середнього розміру,

63
00:02:34,590 --> 00:02:39,900
продуктивність часто дещо краща.

64
00:02:36,330 --> 00:02:42,180
І якщо ми тренуємо дуже велику НМ,

65
00:02:39,900 --> 00:02:44,580
її продуктивність покращується

66
00:02:42,180 --> 00:02:46,890
постійно. Тож, кілька

67
00:02:44,580 --> 00:02:49,410
спостережень. Перше, якщо ми маємо на меті досягти

68
00:02:46,890 --> 00:02:52,620
оцього високого рівня продуктивності, то

69
00:02:49,410 --> 00:02:54,420
нам потрібні дві речі. По-перше, потрібна

70
00:02:52,620 --> 00:02:57,360
можливість тренувати достатньо велику НМ,

71
00:02:54,420 --> 00:02:59,670
щоб отримати користь

72
00:02:57,360 --> 00:03:02,010
від великих об'ємів даних, і, по-друге,

73
00:02:59,670 --> 00:03:05,430
потрібно бути отут на осі x, тобто

74
00:03:02,010 --> 00:03:07,799
мати величезний об'єм даних. Ми часто кажемо,

75
00:03:05,430 --> 00:03:10,860
що розміри рухають прогрес ГН.

76
00:03:07,799 --> 00:03:12,900
І під розмірами я маю на увазі і

77
00:03:10,860 --> 00:03:15,150
розмір НМ (я маю на увазі,

78
00:03:12,900 --> 00:03:17,069
що НМ має багато прихованих вузлів,

79
00:03:15,150 --> 00:03:21,480
багато параметрів, багато зв'язків),

80
00:03:17,069 --> 00:03:23,910
і розмір даних. Фактично,

81
00:03:21,480 --> 00:03:25,440
зараз одним з найбільш надійних способів

82
00:03:23,910 --> 00:03:27,390
отримання вищої продуктивності стає

83
00:03:25,440 --> 00:03:29,940
або натреновувати

84
00:03:27,390 --> 00:03:31,829
більшу мережу, або дати їй більше даних.

85
00:03:29,940 --> 00:03:33,359
Хоча це працює лише до певного моменту,

86
00:03:31,829 --> 00:03:35,640
врешті-решт, у нас або закінчаться дані,

87
00:03:33,359 --> 00:03:37,769
або в якийсь момент НМ стане

88
00:03:35,640 --> 00:03:40,200
настільки великою, що тренуватиметься занадто довго. Бо,

89
00:03:37,769 --> 00:03:42,690
навіть просто підбір розмірів

90
00:03:40,200 --> 00:03:45,810
займає досить багато часу в ГН.

91
00:03:42,690 --> 00:03:48,060
Щоб зробити цей графік дещо

92
00:03:45,810 --> 00:03:49,920
точнішим технічно, давай додамо ще кілька

93
00:03:48,060 --> 00:03:53,040
елементів. Я підписав вісь x

94
00:03:49,920 --> 00:03:57,900
як Кількість даних. Технічно це - об'єм

95
00:03:53,040 --> 00:04:00,180
маркованих даних. А під маркованими даними

96
00:03:57,900 --> 00:04:03,630
я маю на увазі тренувальні зразки, що містять

97
00:04:00,180 --> 00:04:05,910
вхідні x та y. Додам

98
00:04:03,630 --> 00:04:07,709
кілька позначень, які

99
00:04:05,910 --> 00:04:10,769
ми будемо використовувати надалі в цьому курсі. Ми

100
00:04:07,709 --> 00:04:12,540
використовуватимемо маленьку m,

101
00:04:10,769 --> 00:04:13,739
щоб позначати розмір тренувального набору,

102
00:04:12,540 --> 00:04:15,690
тобто кількість тренувальних зразків

103
00:04:13,739 --> 00:04:18,989
маленька m на

104
00:04:15,690 --> 00:04:20,310
горизонтальній осі. І ще кілька деталей

105
00:04:18,989 --> 00:04:23,340
про цей графік.

106
00:04:20,310 --> 00:04:26,970
В цьому діапазоні малих тренувальних наборів

107
00:04:23,340 --> 00:04:29,700
визначити більш продуктивний алгоритм

108
00:04:26,970 --> 00:04:31,590
практично неможливо. Тож, якщо

109
00:04:29,700 --> 00:04:34,500
ми не маємо багато тренувальних даних,

110
00:04:31,590 --> 00:04:36,510
то лише від наших навичок підбору

111
00:04:34,500 --> 00:04:39,090
інженерних показників буде залежати

112
00:04:36,510 --> 00:04:41,910
продуктивність тренування. Тож, достатньо ймовірно, що

113
00:04:39,090 --> 00:04:44,070
якщо один тренер використовує МОВ і

114
00:04:41,910 --> 00:04:46,320
вміє добре підбирати інженерні показники, а

115
00:04:44,070 --> 00:04:48,300
інший тренер буде використовувати велику НМ,

116
00:04:46,320 --> 00:04:50,730
то в діапазоні малих тренувальних наборів

117
00:04:48,300 --> 00:04:53,130
МОВ може мати вищу продуктивність.

118
00:04:50,730 --> 00:04:55,020
Тож, в цьому діапазоні (в лівій

119
00:04:53,130 --> 00:04:57,090
частині графіку) відмінність

120
00:04:55,020 --> 00:04:59,550
між алгоритмами незначна,

121
00:04:57,090 --> 00:05:01,919
і продуктивність більше залежить

122
00:04:59,550 --> 00:05:03,389
від наших навичок підбору інженерних показників

123
00:05:01,919 --> 00:05:05,970
та інших складових

124
00:05:03,389 --> 00:05:08,850
алгоритму. І лише в цьому

125
00:05:05,970 --> 00:05:12,000
діапазоні великих тренувальних наборів,

126
00:05:08,850 --> 00:05:14,669
великого значення m, в правій частині графіку,

127
00:05:12,000 --> 00:05:17,639
ми вочевидь бачимо, що великі НМ

128
00:05:14,669 --> 00:05:19,560
домінують над іншими технологіями, і

129
00:05:17,639 --> 00:05:21,600
коли хтось з твоїх друзів спитає про

130
00:05:19,560 --> 00:05:23,700
перевагу НМ, раджу

131
00:05:21,600 --> 00:05:26,729
намалювати цей графік.

132
00:05:23,700 --> 00:05:28,890
Тож, я б сказав, що

133
00:05:26,729 --> 00:05:29,310
на початку злету сучасного

134
00:05:28,890 --> 00:05:32,070
глибокого навчання

135
00:05:29,310 --> 00:05:34,919
ми мали достатньо даних і достатньо

136
00:05:32,070 --> 00:05:36,330
обчислювальних можливостей, 
тобто сама лише можливість тренувати

137
00:05:34,919 --> 00:05:39,479
великі нейронні мережі

138
00:05:36,330 --> 00:05:41,850
(чи то на центральному процесорі (ЦП) [CPU], 
чи на графічному (ГП) [GPU]) дозволило нам

139
00:05:39,479 --> 00:05:43,590
здійснити значний прогрес. Проте,

140
00:05:41,850 --> 00:05:45,800
особливо за останні

141
00:05:43,590 --> 00:05:48,360
кілька років, ми також побачили значні

142
00:05:45,800 --> 00:05:50,539
алгоритмічні нововведення. Тож я

143
00:05:48,360 --> 00:05:53,700
не хочу занижувати їх значення.

144
00:05:50,539 --> 00:05:56,940
Цікаво, що багато алгоритмічних

145
00:05:53,700 --> 00:06:01,139
нововведень намагались

146
00:05:56,940 --> 00:06:03,510
зробити НМ швидшими. І,

147
00:06:01,139 --> 00:06:05,310
як приклад, одним із значних

148
00:06:03,510 --> 00:06:08,729
проривів у НМ, був

149
00:06:05,310 --> 00:06:12,330
перехід від сигмоїди,

150
00:06:08,729 --> 00:06:14,760
яка виглядає ось так, до ВЛВ [ReLU],

151
00:06:12,330 --> 00:06:18,479
про яку ми коротко говорили

152
00:06:14,760 --> 00:06:20,190
в попередніх відео. Вона виглядає ось так. Якщо

153
00:06:18,479 --> 00:06:22,260
зараз незрозумілі деякі деталі,

154
00:06:20,190 --> 00:06:24,389
не переймайся. Як

155
00:06:22,260 --> 00:06:26,010
виявилось, однією з перешкод

156
00:06:24,389 --> 00:06:27,870
для використання сигмоїди в машинному

157
00:06:26,010 --> 00:06:29,520
навчанні є оцей діапазон,

158
00:06:27,870 --> 00:06:30,280
оцей нахил функції,

159
00:06:29,520 --> 00:06:32,920
де

160
00:06:30,280 --> 00:06:35,350
градієнт є майже нульовим. Тут навчання

161
00:06:32,920 --> 00:06:37,060
стає дуже повільним, тому що при

162
00:06:35,350 --> 00:06:39,639
застосуванні Градієнтного спуску, коли градієнт

163
00:06:37,060 --> 00:06:41,470
рівний нулю, параметри змінюються дуже

164
00:06:39,639 --> 00:06:44,740
повільно і, отже, навчання йде дуже повільно.

165
00:06:41,470 --> 00:06:46,450
Тож, поточну, так звану

166
00:06:44,740 --> 00:06:48,600
функцію активації НМ, вирішили замінити

167
00:06:46,450 --> 00:06:52,060
ось цією. Вона називається

168
00:06:48,600 --> 00:06:54,970
функцією ВЛВ [ReLU], або випрямленим лінійним вузлом.

169
00:06:52,060 --> 00:06:57,070
Тож градієнт дорівнює 1

170
00:06:54,970 --> 00:07:00,220
для всіх вхідних додатних значень,

171
00:06:57,070 --> 00:07:03,100
і, отже,

172
00:07:00,220 --> 00:07:04,750
не наближається до 0, а

173
00:07:03,100 --> 00:07:07,300
будучий нахилом до цієї лінії,

174
00:07:04,750 --> 00:07:09,520
є 0 в лівій частині. Виявляється,

175
00:07:07,300 --> 00:07:12,580
що лише завдяки переходу від сигмоїди

176
00:07:09,520 --> 00:07:14,410
до функції ВЛВ [ReLU]

177
00:07:12,580 --> 00:07:16,960
алгоритм, що називається Градієнтний спуск,

178
00:07:14,410 --> 00:07:19,169
почав працювати набагато швидше. Тож це -

179
00:07:16,960 --> 00:07:22,030
приклад, можливо, відносно простого

180
00:07:19,169 --> 00:07:23,860
нововведення в алгоритмах проте, насправді,

181
00:07:22,030 --> 00:07:27,520
воно мало значний вплив на алгоритмічні нововведення

182
00:07:23,860 --> 00:07:29,080
і дуже допомогло обчисленням. Тож,

183
00:07:27,520 --> 00:07:31,240
є багато схожих прикладів,

184
00:07:29,080 --> 00:07:33,340
де ми змінили алгоритм,

185
00:07:31,240 --> 00:07:35,140
і тому це дозволило виконувати код

186
00:07:33,340 --> 00:07:37,479
набагато швидше. Це дозволило нам тренувати

187
00:07:35,140 --> 00:07:39,520
більші нейронні мережі,

188
00:07:37,479 --> 00:07:42,250
тобто отримати значний виграш в часі, навіть якщо ми маємо

189
00:07:39,520 --> 00:07:45,810
велику мережу або багато даних.

190
00:07:42,250 --> 00:07:48,610
Інша причина, яка прискорила обчислення,

191
00:07:45,810 --> 00:07:51,070
теж досить важлива. Виявляється,

192
00:07:48,610 --> 00:07:53,710
що процес тренування НМ є

193
00:07:51,070 --> 00:07:56,350
дуже циклічним. Часто є просто ідея

194
00:07:53,710 --> 00:07:58,020
архітектури НМ, потім

195
00:07:56,350 --> 00:08:01,060
вона реалізується в коді.

196
00:07:58,020 --> 00:08:02,830
Впровадження ідеї дозволяє запустити

197
00:08:01,060 --> 00:08:05,050
експеримент, який покаже наскільки добре працює

198
00:08:02,830 --> 00:08:07,510
наша НМ, а потім,

199
00:08:05,050 --> 00:08:10,030
проаналізувавши, можеш змінити

200
00:08:07,510 --> 00:08:12,930
деякі параметри нейронної мережі, і потім

201
00:08:10,030 --> 00:08:15,880
рухатись вздовж цього циклу знову і знову.

202
00:08:12,930 --> 00:08:18,550
А коли твоя нейронна мережа дуже довго

203
00:08:15,880 --> 00:08:21,400
тренується, то по цьому цикл просувається

204
00:08:18,550 --> 00:08:24,039
дуже повільно. Відчувається значна

205
00:08:21,400 --> 00:08:26,740
різниця в продуктивності при побудові

206
00:08:24,039 --> 00:08:29,560
ефективних нейронних мереж, бо

207
00:08:26,740 --> 00:08:34,169
від ідеї до випробувань проходить

208
00:08:29,560 --> 00:08:36,370
кілька хвилин або більше за один день,

209
00:08:34,169 --> 00:08:39,490
і коли тренуєш нейронну мережу

210
00:08:36,370 --> 00:08:40,590
цілий місяць, що інколи

211
00:08:39,490 --> 00:08:42,570
трапляється.

212
00:08:40,590 --> 00:08:44,670
Коли отримуєш результат

213
00:08:42,570 --> 00:08:47,250
через 10 хвилин чи того ж дня,

214
00:08:44,670 --> 00:08:49,170
можеш реалізувати більше ідей і

215
00:08:47,250 --> 00:08:50,610
найбільш ймовірно, знайти нейронну мережу

216
00:08:49,170 --> 00:08:53,720
найбільш відповідну до вимог

217
00:08:50,610 --> 00:08:57,900
програмного додатку. Також швидші обчислення

218
00:08:53,720 --> 00:08:59,730
допомогли з точки зору прискорення

219
00:08:57,900 --> 00:09:02,610
отримання

220
00:08:59,730 --> 00:09:05,400
результатів експериментів. Це

221
00:09:02,610 --> 00:09:07,550
допомогло і практикам нейронних мереж,

222
00:09:05,400 --> 00:09:10,650
і дослідникам глибинного навчання,

223
00:09:07,550 --> 00:09:13,320
дало змогу ітерувати набагато

224
00:09:10,650 --> 00:09:16,589
швидше і вдосконалювати ідеї набагато

225
00:09:13,320 --> 00:09:18,570
швидше. Отже, все це

226
00:09:16,589 --> 00:09:21,029
мало значний вплив на всю

227
00:09:18,570 --> 00:09:23,370
спільноту дослідників глибинного навчання.

228
00:09:21,029 --> 00:09:25,620
Неймовірно захоплюючим процесом є винахід

229
00:09:23,370 --> 00:09:28,920
нових алгоритмів і створювання невпинного

230
00:09:25,620 --> 00:09:30,990
прогресу на фронті глибинного навчання. Отже, я назвав

231
00:09:28,920 --> 00:09:33,570
рушійні сили злету глибинного навчання.

232
00:09:30,990 --> 00:09:36,000
І це ще не все, ми маємо добрі новини, що

233
00:09:33,570 --> 00:09:38,490
ці сили досі потужно діють, щоб

234
00:09:36,000 --> 00:09:41,130
зробити глибинне навчання ще кращим. Завдяки технічним даним

235
00:09:38,490 --> 00:09:43,800
(суспільство генерує все більше

236
00:09:41,130 --> 00:09:45,660
цифрових даних), а також зросту швидкості обчислення

237
00:09:43,800 --> 00:09:48,300
завдяки апаратному забезпеченню, (наприклад

238
00:09:45,660 --> 00:09:50,940
GPU, збільшення пропускної здатності мереж

239
00:09:48,300 --> 00:09:53,250
та іншому апаратному забезпеченню) я впевнений,

240
00:09:50,940 --> 00:09:55,140
що можливість будувати дуже великі нейронні мережі

241
00:09:53,250 --> 00:09:57,320
(з точки зору обчислень)

242
00:09:55,140 --> 00:10:00,360
буде лише збільшуватись.

243
00:09:57,320 --> 00:10:02,880
А що до алгоритмів,

244
00:10:00,360 --> 00:10:05,070
то спільнота дослідників глибинного навчання постійно

245
00:10:02,880 --> 00:10:07,680
і феноменально працює

246
00:10:05,070 --> 00:10:09,839
і на цьому фронті. Тож через це

247
00:10:07,680 --> 00:10:11,370
я думаю, що ми можемо бути оптимістами.

248
00:10:09,839 --> 00:10:13,650
Я теж оптиміст. Я думаю, що глибинне навчання

249
00:10:11,370 --> 00:10:14,120
покращуватиметься

250
00:10:13,650 --> 00:10:17,100
впродовж багатьох років.

251
00:10:14,120 --> 00:10:18,540
Тож, давай перейдемо до останнього відео

252
00:10:17,100 --> 00:10:20,280
в цьому розділі, в якому ми

253
00:10:18,540 --> 00:10:22,610
деталізуємо те, про що спілкуватимемось

254
00:10:20,280 --> 00:10:22,610
у цьому курсі.