Якщо технічні ідеї, на яких базується глибинне навчання (ГН) і нейронні мережі (НМ), витають в повітрі ще з минулого століття, то чому ж вони розвинулись лише зараз? В цьому відео давай розглянемо головні рушійні сили розквіту ГН, бо я думаю, що це допоможе твоїй команді вибрати те, що краще застосувати. Протягом останніх кілька років багато людей питали мене: "Андрію, чому глибинне навчання так добре працює?". І коли я відповідаю на це питання, зазвичай, показую оце зображення. Скажімо, ми намалювали графік, де горизонтальна вісь відповідає за кількість даних, які нам дано для задачі, і, скажімо, вертикальна вісь відображає продуктивність навчальних алгоритмів, про які ми говорили раніше, 
наприклад, точність класифікатора спаму, або точність передбачення клацання по рекламі, або точність вказання НМ-ою позицій інших авто для безпілотного авто. Виявляється, якщо розглядати продуктивність традиційного навчального алгоритму, такого як метод опорних векторів (МОВ) або логістична регресія, як функцію від кількості наявних даних, то можна отримати приблизно ось таку криву, де продуктивність недовго збільшується, якщо збільшується кількість даних, але в певний момент переходить в плато. (Це мала б бути горизонтальна, проте, я не дуже добре її намалював). Вони не знають, що робити з великими об'ємами даних. В нашому суспільстві протягом останніх, можливо, 20 років для багатьох задач замість відносно малих об'ємів даних ми отримали часто досить великі об'єми даних. І багато з них - завдяки оцифруванню суспільства. Зараз дуже багато людської активності знаходиться в цифровому царстві. Ми проводимо дуже багато часу перед комп'ютерами, на сайтах, у мобільних додатках, і своєю активністю на цифрових пристроях створюємо дані. Завдяки поширенню недорогих камер, вбудованих в наші мобільні телефони, акселерометрам та іншим видам сенсорів в Інтернеті речей, ми також накопичуємо все більше і більше даних. Тож протягом останніх 20 років для багатьох задач ми накопичили об'єми даних більші, ніж традиційні навчальні алгоритми в змозі ефективно використовувати. А з допомогою НМ, виявляється, що у випадку тренування маленької НМ крива продуктивності буде приблизно ось такою. Якщо ми тренуємо трохи більшу НМ, назвемо її НМ середнього розміру, продуктивність часто дещо краща. І якщо ми тренуємо дуже велику НМ, її продуктивність покращується постійно. Тож, кілька спостережень. Перше, якщо ми маємо на меті досягти оцього високого рівня продуктивності, то нам потрібні дві речі. По-перше, потрібна можливість тренувати достатньо велику НМ, щоб отримати користь від великих об'ємів даних, і, по-друге, потрібно бути отут на осі x, тобто мати величезний об'єм даних. Ми часто кажемо, що розміри рухають прогрес ГН. І під розмірами я маю на увазі і розмір НМ (я маю на увазі, що НМ має багато прихованих вузлів, багато параметрів, багато зв'язків), і розмір даних. Фактично, зараз одним з найбільш надійних способів отримання вищої продуктивності стає або натреновувати більшу мережу, або дати їй більше даних. Хоча це працює лише до певного моменту, врешті-решт, у нас або закінчаться дані, або в якийсь момент НМ стане настільки великою, що тренуватиметься занадто довго. Бо, навіть просто підбір розмірів займає досить багато часу в ГН. Щоб зробити цей графік дещо точнішим технічно, давай додамо ще кілька елементів. Я підписав вісь x як Кількість даних. Технічно це - об'єм маркованих даних. А під маркованими даними я маю на увазі тренувальні зразки, що містять вхідні x та y. Додам кілька позначень, які ми будемо використовувати надалі в цьому курсі. Ми використовуватимемо маленьку m, щоб позначати розмір тренувального набору, тобто кількість тренувальних зразків маленька m на горизонтальній осі. І ще кілька деталей про цей графік. В цьому діапазоні малих тренувальних наборів визначити більш продуктивний алгоритм практично неможливо. Тож, якщо ми не маємо багато тренувальних даних, то лише від наших навичок підбору інженерних показників буде залежати продуктивність тренування. Тож, достатньо ймовірно, що якщо один тренер використовує МОВ і вміє добре підбирати інженерні показники, а інший тренер буде використовувати велику НМ, то в діапазоні малих тренувальних наборів МОВ може мати вищу продуктивність. Тож, в цьому діапазоні (в лівій частині графіку) відмінність між алгоритмами незначна, і продуктивність більше залежить від наших навичок підбору інженерних показників та інших складових алгоритму. І лише в цьому діапазоні великих тренувальних наборів, великого значення m, в правій частині графіку, ми вочевидь бачимо, що великі НМ домінують над іншими технологіями, і коли хтось з твоїх друзів спитає про перевагу НМ, раджу намалювати цей графік. Тож, я б сказав, що на початку злету сучасного глибокого навчання ми мали достатньо даних і достатньо обчислювальних можливостей, 
тобто сама лише можливість тренувати великі нейронні мережі (чи то на центральному процесорі (ЦП) [CPU], 
чи на графічному (ГП) [GPU]) дозволило нам здійснити значний прогрес. Проте, особливо за останні кілька років, ми також побачили значні алгоритмічні нововведення. Тож я не хочу занижувати їх значення. Цікаво, що багато алгоритмічних нововведень намагались зробити НМ швидшими. І, як приклад, одним із значних проривів у НМ, був перехід від сигмоїди, яка виглядає ось так, до ВЛВ [ReLU], про яку ми коротко говорили в попередніх відео. Вона виглядає ось так. Якщо зараз незрозумілі деякі деталі, не переймайся. Як виявилось, однією з перешкод для використання сигмоїди в машинному навчанні є оцей діапазон, оцей нахил функції, де градієнт є майже нульовим. Тут навчання стає дуже повільним, тому що при застосуванні Градієнтного спуску, коли градієнт рівний нулю, параметри змінюються дуже повільно і, отже, навчання йде дуже повільно. Тож, поточну, так звану функцію активації НМ, вирішили замінити ось цією. Вона називається функцією ВЛВ [ReLU], або випрямленим лінійним вузлом. Тож градієнт дорівнює 1 для всіх вхідних додатних значень, і, отже, не наближається до 0, а будучий нахилом до цієї лінії, є 0 в лівій частині. Виявляється, що лише завдяки переходу від сигмоїди до функції ВЛВ [ReLU] алгоритм, що називається Градієнтний спуск, почав працювати набагато швидше. Тож це - приклад, можливо, відносно простого нововведення в алгоритмах проте, насправді, воно мало значний вплив на алгоритмічні нововведення і дуже допомогло обчисленням. Тож, є багато схожих прикладів, де ми змінили алгоритм, і тому це дозволило виконувати код набагато швидше. Це дозволило нам тренувати більші нейронні мережі, тобто отримати значний виграш в часі, навіть якщо ми маємо велику мережу або багато даних. Інша причина, яка прискорила обчислення, теж досить важлива. Виявляється, що процес тренування НМ є дуже циклічним. Часто є просто ідея архітектури НМ, потім вона реалізується в коді. Впровадження ідеї дозволяє запустити експеримент, який покаже наскільки добре працює наша НМ, а потім, проаналізувавши, можеш змінити деякі параметри нейронної мережі, і потім рухатись вздовж цього циклу знову і знову. А коли твоя нейронна мережа дуже довго тренується, то по цьому цикл просувається дуже повільно. Відчувається значна різниця в продуктивності при побудові ефективних нейронних мереж, бо від ідеї до випробувань проходить кілька хвилин або більше за один день, і коли тренуєш нейронну мережу цілий місяць, що інколи трапляється. Коли отримуєш результат через 10 хвилин чи того ж дня, можеш реалізувати більше ідей і найбільш ймовірно, знайти нейронну мережу найбільш відповідну до вимог програмного додатку. Також швидші обчислення допомогли з точки зору прискорення отримання результатів експериментів. Це допомогло і практикам нейронних мереж, і дослідникам глибинного навчання, дало змогу ітерувати набагато швидше і вдосконалювати ідеї набагато швидше. Отже, все це мало значний вплив на всю спільноту дослідників глибинного навчання. Неймовірно захоплюючим процесом є винахід нових алгоритмів і створювання невпинного прогресу на фронті глибинного навчання. Отже, я назвав рушійні сили злету глибинного навчання. І це ще не все, ми маємо добрі новини, що ці сили досі потужно діють, щоб зробити глибинне навчання ще кращим. Завдяки технічним даним (суспільство генерує все більше цифрових даних), а також зросту швидкості обчислення завдяки апаратному забезпеченню, (наприклад GPU, збільшення пропускної здатності мереж та іншому апаратному забезпеченню) я впевнений, що можливість будувати дуже великі нейронні мережі (з точки зору обчислень) буде лише збільшуватись. А що до алгоритмів, то спільнота дослідників глибинного навчання постійно і феноменально працює і на цьому фронті. Тож через це я думаю, що ми можемо бути оптимістами. Я теж оптиміст. Я думаю, що глибинне навчання покращуватиметься впродовж багатьох років. Тож, давай перейдемо до останнього відео в цьому розділі, в якому ми деталізуємо те, про що спілкуватимемось у цьому курсі.