1
00:00:00,390 --> 00:00:04,350
जब आप बनाते हैं एक न्यूरल नेटवर्क

2
00:00:02,580 --> 00:00:06,720
एक विकल्प जो आपको चुनना होता है कि क्या

3
00:00:04,350 --> 00:00:09,599
ऐक्टिवेशन फ़ंक्शन इस्तेमाल करना है हिडन

4
00:00:06,720 --> 00:00:11,490
लेयर्स में तथा आउट्पुट यूनिट में

5
00:00:09,599 --> 00:00:13,139
आपके न्यूरल नेटवर्क में. अभी तक हम केवल

6
00:00:11,490 --> 00:00:16,080
इस्तेमाल करते रहे हैं सिग्मोईड ऐक्टिवेशन

7
00:00:13,139 --> 00:00:18,720
फ़ंक्शन लेकिन कभी कभी अन्य विकल्प 
कर सकते हैं

8
00:00:16,080 --> 00:00:20,939
बेहतर काम. चलो देखते हैं

9
00:00:18,720 --> 00:00:23,279
कुछ विकल्प फ़ॉर्वर्ड

10
00:00:20,939 --> 00:00:26,099
प्रॉपगेशन स्टेप में न्यूरल नेटवर्क में.

11
00:00:23,279 --> 00:00:28,710
हमारे पास हैं ये दो स्टेप्स जहाँ हम इस्तेमाल 
करते हैं

12
00:00:26,099 --> 00:00:32,610
सिग्मोईड फ़ंक्शन यहाँ तो उस सिग्मोईड को

13
00:00:28,710 --> 00:00:37,590
कहते हैं एक ऐक्टिवेशन फ़ंक्शन और यहाँ है

14
00:00:32,610 --> 00:00:40,680
परिचित सिग्मोईड फ़ंक्शन a बराबर 1

15
00:00:37,590 --> 00:00:42,600
ओवर 1 प्लस e की पॉवर माइनस z तो

16
00:00:40,680 --> 00:00:49,739
अधिक सामान्य केस में हमारे पास 
हो सकता है एक

17
00:00:42,600 --> 00:00:53,309
फ़ंक्शन G, z का. चलो लिखते हैं इसे

18
00:00:49,739 --> 00:00:56,010
यहाँ जहाँ G हो सकता हैं एक नॉन-लिनियर

19
00:00:53,309 --> 00:00:59,250
फ़ंक्शन जो हो सकता है न हो सिग्मोईड

20
00:00:56,010 --> 00:01:01,879
फ़ंक्शन. तो उदाहरण के लिए सिग्मोईड

21
00:00:59,250 --> 00:01:04,290
फ़ंक्शन होता है 0 और 1 के बीच. एक

22
00:01:01,879 --> 00:01:06,900
ऐक्टिवेशन फ़ंक्शन जो लगभग हमेशा

23
00:01:04,290 --> 00:01:10,320
बेहतर काम करता है सिग्मोईड फ़ंक्शन से

24
00:01:06,900 --> 00:01:14,189
है tanh फ़ंक्शन या हायपर्बालिक

25
00:01:10,320 --> 00:01:19,979
टैंजेंट फ़ंक्शन तो यह है G. यह है a

26
00:01:14,189 --> 00:01:25,710
यह है a बराबर tanh z का और यह जाता है

27
00:01:19,979 --> 00:01:31,079
प्लस 1 और माइनस 1 के बीच. फ़ॉर्म्युला

28
00:01:25,710 --> 00:01:37,799
tanh फ़ंक्शन का है e की पॉवर z माइनस

29
00:01:31,079 --> 00:01:40,140
e की पॉवर नेगेटिव z ओवर उनका योग और 
यह है

30
00:01:37,799 --> 00:01:43,890
वास्तव में गणितीय रूप से एक शिफ़्ट किया हुआ

31
00:01:40,140 --> 00:01:46,350
वर्ज़न सिग्मोईड फ़ंक्शन का तो जैसे एक

32
00:01:43,890 --> 00:01:49,860
आप जानते हैं सिग्मोईड फ़ंक्शन बस उसके जैसा

33
00:01:46,350 --> 00:01:52,079
लेकिन शिफ़्ट किया हुआ ताकि अब यह
 क्रॉस करता है

34
00:01:49,860 --> 00:01:54,570
ज़ीरो, ज़ीरो पोईँट और दोबारा स्केल करके 
ताकि यह जाता है

35
00:01:52,079 --> 00:01:58,530
G पर माइनस एक और प्लस एक पर और ऐसा

36
00:01:54,570 --> 00:02:05,340
होता है के हिडन यूनिट्स के लिए यदि आप 
रखते हैं

37
00:01:58,530 --> 00:02:09,910
फ़ंक्शन G, z का बराबर

38
00:02:05,340 --> 00:02:12,490
z के tanh के, यह लगभग हमेशा 
बेहतर काम करता है

39
00:02:09,910 --> 00:02:14,020
सिग्मोईड फ़ंक्शन की तुलना में क्योंकि

40
00:02:12,490 --> 00:02:16,930
माइनस एक और प्लस एक के बीच की वैल्यूज़ से

41
00:02:14,020 --> 00:02:19,000
ऐक्टिवेशन्स की औसत जो आती है

42
00:02:16,930 --> 00:02:21,550
हिडन लेयर्स से वहाँ है क़रीब

43
00:02:19,000 --> 00:02:23,020
ज़ीरो होने के और बस जैसे

44
00:02:21,550 --> 00:02:23,590
कभी-कभी आप ट्रेन करते हैं एक लर्निंग

45
00:02:23,020 --> 00:02:25,690
अल्गोरिद्म

46
00:02:23,590 --> 00:02:29,709
आप डेटा को केंद्रित कर सकते हैं 
और कर सकते हैं आपका

47
00:02:25,690 --> 00:02:31,510
डेटा ज़ीरो औसत वाला इस्तेमाल करके 
एक tanh बजाय

48
00:02:29,709 --> 00:02:34,750
एक सिग्मोईड फ़ंक्शन के. इसका एक तरह से

49
00:02:31,510 --> 00:02:36,880
प्रभाव है केंद्रित करना आपका डेटा ताकि

50
00:02:34,750 --> 00:02:39,610
डेटा की औसत है क़रीब

51
00:02:36,880 --> 00:02:41,410
ज़ीरो के बजाय शायद एक 0.5 के और यह

52
00:02:39,610 --> 00:02:43,510
वास्तव में बनाता है लर्निंग अगली

53
00:02:41,410 --> 00:02:45,820
लेयर के लिए थोड़ा आसान. हम कहेंगे अधिक

54
00:02:43,510 --> 00:02:47,380
इस बारे में दूसरे कोर्स में जब हम

55
00:02:45,820 --> 00:02:50,739
बात करेंगे ऑप्टिमायज़ेशन अल्गोरिद्म्स की भी.

56
00:02:47,380 --> 00:02:52,480
लेकिन एक चीज़ समझने की है कि मैं लगभग

57
00:02:50,739 --> 00:02:54,250
कभी भी इस्तेमाल नहीं करता सिग्मोईड ऐक्टिवेशन

58
00:02:52,480 --> 00:02:56,410
फ़ंक्शन अब.

59
00:02:54,250 --> 00:02:59,560
tanh फ़ंक्शन है लगभग हमेशा

60
00:02:56,410 --> 00:03:03,550
काफ़ी बेहतर. एक अपवाद है

61
00:02:59,560 --> 00:03:07,420
आउट्पुट लेयर के लिए, क्योंकि यदि Y है

62
00:03:03,550 --> 00:03:10,570
0 या 1 तब यह बेहतर लगता है कि y

63
00:03:07,420 --> 00:03:13,989
हैट हो एक नम्बर जो आप चाहते हैं

64
00:03:10,570 --> 00:03:16,570
आउट्पुट करना 0 और 1 के बीच बजाय

65
00:03:13,989 --> 00:03:19,360
प्लस 1 और माइनस 1 के बीच. तो एक

66
00:03:16,570 --> 00:03:21,430
अपवाद जहाँ मैं इस्तेमाल करूँगा सिग्मोईड

67
00:03:19,360 --> 00:03:24,670
ऐक्टिवेशन फ़ंक्शन जहाँ आप इस्तेमाल कर रहे हैं

68
00:03:21,430 --> 00:03:26,350
बाइनरी वर्गीकरण उस स्थिति में आप

69
00:03:24,670 --> 00:03:29,709
शायद इस्तेमाल करें सिग्मोईड ऐक्टिवेशन

70
00:03:26,350 --> 00:03:35,170
फ़ंक्शन आउट्पुट लेयर के लिए. तो z का G

71
00:03:29,709 --> 00:03:37,180
2 यहाँ है बराबर सिग्मोईड z का 2 और इसलिए

72
00:03:35,170 --> 00:03:40,299
जो आप इस उदाहरण में है जहां

73
00:03:37,180 --> 00:03:43,920
आपके पास है शायद एक tanh ऐक्टिवेशन

74
00:03:40,299 --> 00:03:47,769
फ़ंक्शन हिडन लेयर के लिए और

75
00:03:43,920 --> 00:03:49,299
सिग्मोईड फ़ंक्शन आउट्पुट लेयर के लिए. तो

76
00:03:47,769 --> 00:03:51,670
ऐक्टिवेशन फ़ंक्शन्स भिन्न हो सकते हैं

77
00:03:49,299 --> 00:03:53,709
भिन्न लेयर्स के लिए और कभी-कभी

78
00:03:51,670 --> 00:03:55,690
डिनोट करने के लिए कि ऐक्टिवेशन फ़ंक्शन्स है

79
00:03:53,709 --> 00:03:58,510
भिन्न भिन्न लेयर्स के लिए हम शायद

80
00:03:55,690 --> 00:04:02,230
उपयोग करें इन वर्ग कोष्ठक सूपरस्क्रिप्ट्स का

81
00:03:58,510 --> 00:04:04,540
भी इंगित करने के लिए कि G वर्ग

82
00:04:02,230 --> 00:04:06,940
कोष्ठक 1 है भिन्न G वर्ग

83
00:04:04,540 --> 00:04:09,340
कोष्ठक 2 से. ठीक है, फिर से वर्ग कोष्ठक 1

84
00:04:06,940 --> 00:04:11,470
सूपरस्क्रिप्ट संदर्भित करता है इस लेयर को और

85
00:04:09,340 --> 00:04:12,879
सुपरस्क्रिप्ट वर्ग कोष्ठक 2 संदर्भित करता है

86
00:04:11,470 --> 00:04:15,680
आउट्पुट लेयर को.

87
00:04:12,879 --> 00:04:18,109
अब एक नकारात्मक पहलू दोनो

88
00:04:15,680 --> 00:04:20,780
सिग्मोईड फ़ंक्शन और tanh फ़ंक्शन का है

89
00:04:18,109 --> 00:04:22,910
कि यदि z है या बहुत बड़ा या बहुत

90
00:04:20,780 --> 00:04:24,460
छोटा तब ग्रेडीयंट

91
00:04:22,910 --> 00:04:27,560
डेरिवेटिव का या इस फ़ंक्शन की स्लोप

92
00:04:24,460 --> 00:04:30,139
हो जाती है बहुत छोटी तो z है बहुत बड़ा या

93
00:04:27,560 --> 00:04:33,169
z है बहुत छोटा तो स्लोप

94
00:04:30,139 --> 00:04:35,270
फ़ंक्शन की आप जानते हैं हो जाती है क़रीब

95
00:04:33,169 --> 00:04:38,360
ज़ीरो और इसलिए यह धीमा कर सकता है ग्रेडीयंट

96
00:04:35,270 --> 00:04:41,810
डिसेंट. तो एक विकल्प जो बहुत

97
00:04:38,360 --> 00:04:44,900
लोकप्रिय है मशीन लर्निंग में है जिसे

98
00:04:41,810 --> 00:04:50,720
कहते हैं रेक्टिफ़ायड लिनियर यूनिट तो

99
00:04:44,900 --> 00:04:57,110
ReLU फ़ंक्शन ऐसा दिखता हैं और

100
00:04:50,720 --> 00:05:00,500
फ़ॉर्म्युला है a बराबर मैक्स ऑफ़ 0 कॉमा z इसलिए

101
00:04:57,110 --> 00:05:03,530
डेरिवेटिव है 1 जब तक z है

102
00:05:00,500 --> 00:05:05,990
पॉज़िटिव और डेरिवेटिव या स्लोप है

103
00:05:03,530 --> 00:05:07,580
0 जब z है नेगेटिव. यदि आप

104
00:05:05,990 --> 00:05:10,190
इम्प्लमेंट कर रहे हैं इसे तकनीकी रूप से

105
00:05:07,580 --> 00:05:12,349
डेरिवेटिव जब Z है पूर्णत: 0 नहीं है

106
00:05:10,190 --> 00:05:14,210
परिभाषित लेकिन जब आप इम्प्लमेंट करते हैं इसे

107
00:05:12,349 --> 00:05:18,770
कम्प्यूटर में तब अक्सर आपको मिलेगा यह

108
00:05:14,210 --> 00:05:21,229
z पूर्णत:बराबर 0 0 0 0 0 0 0 0 0 0.

109
00:05:18,770 --> 00:05:22,940
यह बहुत ही कम है तो आपको ज़रूरत नहीं है

110
00:05:21,229 --> 00:05:25,610
चिंता करने की इसकी. व्यावहारिक रूप से आप

111
00:05:22,940 --> 00:05:29,659
ले सकते हैं एक डेरिवेटिव जब z है बराबर

112
00:05:25,610 --> 00:05:32,270
0 आप इसे ले सकते हैं 1 या 0 और

113
00:05:29,659 --> 00:05:35,479
आपका काम होगा सही. तो तथ्य कि

114
00:05:32,270 --> 00:05:37,430
इसे ड़िफ़्फ़ेरेंशिएट नहीं कर सकते तो

115
00:05:35,479 --> 00:05:40,010
यहाँ हैं कुछ अनुभवसिद्ध नियम

116
00:05:37,430 --> 00:05:43,280
चुनने के लिए ऐक्टिवेशन फ़ंक्शन्स यदि आपकी

117
00:05:40,010 --> 00:05:45,620
आउट्पुट है 0 1 वैल्यू यदि आप इस्तेमाल कर रहे हैं

118
00:05:43,280 --> 00:05:47,539
बाइनरी वर्गीकरण तब सिग्मोईड

119
00:05:45,620 --> 00:05:50,479
ऐक्टिवेशन फ़ंक्शन बहुत स्वभाविक है

120
00:05:47,539 --> 00:05:59,419
आउट्पुट लेयर के लिए और तब बाक़ी सभी

121
00:05:50,479 --> 00:06:04,460
यूनिट्स के लिए ReLU या रेक्टिफ़ायड लिनियर

122
00:05:59,419 --> 00:06:07,190
यूनिट बन रहा है तेजी से डिफ़ॉल्ट विकल्प

123
00:06:04,460 --> 00:06:10,280
ऐक्टिवेशन फ़ंक्शन का तो यदि आप नहीं हैं

124
00:06:07,190 --> 00:06:13,849
सुनिश्चित कि क्या इस्तेमाल करें 
आपकी हिडन लेयर के लिए

125
00:06:10,280 --> 00:06:15,289
मैं बस इस्तेमाल करूँगा ReLU ऐक्टिवेशन

126
00:06:13,849 --> 00:06:17,570
फ़ंक्शन. वह है जो आप देखेंगे अधिकांश लोग

127
00:06:15,289 --> 00:06:20,120
इस्तेमाल करते हैं इन दिनो जबकि कभी-कभी

128
00:06:17,570 --> 00:06:21,350
लोग इस्तेमाल करते हैं tanh ऐक्टिवेशन

129
00:06:20,120 --> 00:06:23,150
फ़ंक्शन भी.

130
00:06:21,350 --> 00:06:26,270
एक समस्या ReLU की है कि

131
00:06:23,150 --> 00:06:28,640
डेरिवेटिव है बराबर ज़ीरो के जब z

132
00:06:26,270 --> 00:06:31,700
है नेगेटिव. व्यावहारिक रूप से यह काम करता है

133
00:06:28,640 --> 00:06:33,890
सही लेकिन वहाँ एक अन्य वर्ज़न है

134
00:06:31,700 --> 00:06:35,420
ReLU का जिसे कहते हैं leaky ReLU. दूँगा

135
00:06:33,890 --> 00:06:38,690
आपको फ़ॉर्म्युला अगली स्लाइड पर लेकिन

136
00:06:35,420 --> 00:06:40,520
बजाय इसके होने के ज़ीरो जब z है

137
00:06:38,690 --> 00:06:42,940
नेगेटिव यह सिर्फ़ लेता है एक थोड़ी स्लोप

138
00:06:40,520 --> 00:06:47,900
इस तरह. तो इसलिए इसे कहते हैं leaky

139
00:06:42,940 --> 00:06:51,170
ReLU. यह अक्सर बेहतर काम करता है तुलना में

140
00:06:47,900 --> 00:06:53,900
ReLU ऐक्टिवेशन फ़ंक्शन के हालाँकि यह

141
00:06:51,170 --> 00:06:54,860
सिर्फ़ उतना इस्तेमाल नहीं होता अभ्यास में. 
कोई भी

142
00:06:53,900 --> 00:06:56,770
दोनो में से ठीक होना चाहिए

143
00:06:54,860 --> 00:06:59,330
हालांकि अगर आपको एक लेना होता मैं

144
00:06:56,770 --> 00:07:01,460
आमतौर पर लेता ReLU और

145
00:06:59,330 --> 00:07:04,460
लाभ दोनो ReLU और leaky ReLU

146
00:07:01,460 --> 00:07:06,500
का है कि बहुत सी स्पेस

147
00:07:04,460 --> 00:07:08,150
z की, डेरिवेटिव ऐक्टिवेशन

148
00:07:06,500 --> 00:07:11,870
फ़ंक्शन का, स्लोप ऐक्टिवेशन

149
00:07:08,150 --> 00:07:13,970
फ़ंक्शन की है बहुत अलग ज़ीरो से और

150
00:07:11,870 --> 00:07:15,920
इसलिए अभ्यास में इस्तेमाल करना ReLU

151
00:07:13,970 --> 00:07:18,590
ऐक्टिवेशन फ़ंक्शन आपका न्यूरल नेटवर्क

152
00:07:15,920 --> 00:07:20,810
अक्सर लर्न करेगा बहुत जल्दी तुलना में 
इस्तेमाल करने से

153
00:07:18,590 --> 00:07:23,840
tanh या सिग्मोईड ऐक्टिवेशन

154
00:07:20,810 --> 00:07:26,420
फ़ंक्शन के और मुख्य कारण है कि

155
00:07:23,840 --> 00:07:28,700
वहाँ इसका प्रभाव कम है स्लोप पर

156
00:07:26,420 --> 00:07:31,580
फ़ंक्शन के ज़ीरो होने पर जिससे धीरे

157
00:07:28,700 --> 00:07:33,950
हो जाती है लर्निंग और मैं जानता हूँ कि आधी

158
00:07:31,580 --> 00:07:36,710
रेंज के लिए z की स्लोप ReLU की है

159
00:07:33,950 --> 00:07:39,050
ज़ीरो लेकिन अभ्यास में पर्याप्त आपकी

160
00:07:36,710 --> 00:07:41,120
हिडन यूनिट्स में होगा z बड़ा

161
00:07:39,050 --> 00:07:43,700
ज़ीरो से तो लर्निंग रेट हो सकती है काफ़ी अधिक

162
00:07:41,120 --> 00:07:45,800
बहुत से ट्रेनिंग इग्ज़ाम्पल्ज़ के लिए. तो चलो

163
00:07:43,700 --> 00:07:47,600
जल्दी से देखते हैं लाभ और हानियाँ

164
00:07:45,800 --> 00:07:50,030
विभिन्न ऐक्टिवेशन फ़ंक्शन्स की. यहाँ है

165
00:07:47,600 --> 00:07:52,790
सिग्मोईड ऐक्टिवेशन मैं

166
00:07:50,030 --> 00:07:54,410
कहूँगा कभी इस्तेमाल न करें सिवाय आउट्पुट

167
00:07:52,790 --> 00:07:56,330
लेयर के यदि आप कर रहे हैं बाइनरी

168
00:07:54,410 --> 00:07:59,540
वर्गीकरण या शायद कभी न इस्तेमाल करें

169
00:07:56,330 --> 00:08:02,720
इसका और कारण कि मैं लगभग कभी 
उपयोग नहीं करता

170
00:07:59,540 --> 00:08:05,060
इसका है क्योंकि tanh है बहुत

171
00:08:02,720 --> 00:08:12,080
अधिक बेहतर. तो tanh

172
00:08:05,060 --> 00:08:13,430
ऐक्टिवेशन फ़ंक्शन है यह और फिर

173
00:08:12,080 --> 00:08:15,490
डिफ़ॉल्ट, सबसे अधिक इस्तेमाल होने वाला

174
00:08:13,430 --> 00:08:19,100
ऐक्टिवेशन फ़ंक्शन है ReLU

175
00:08:15,490 --> 00:08:23,660
जो है यह. तो आप सुनिश्चित नहीं है क्या

176
00:08:19,100 --> 00:08:26,600
और इस्तेमाल कर सकते हैं, इस्तेमाल करें इसे 
और शायद आप

177
00:08:23,660 --> 00:08:31,930
जानते हैं बेझिझक इस्तेमाल करें leaky ReLU.

178
00:08:26,600 --> 00:08:36,659
जहाँ शायद

179
00:08:31,930 --> 00:08:40,390
0.01 z कॉमा z, ठीक है तो a है मैक्स

180
00:08:36,659 --> 00:08:43,810
0.01 गुणा z और z का. तो वह देता है आपको

181
00:08:40,390 --> 00:08:46,200
यह मोड़ फ़ंक्शन में और आप

182
00:08:43,810 --> 00:08:51,670
शायद कहें आप जानते हैं क्यों है वह कॉन्स्टंट

183
00:08:46,200 --> 00:08:53,380
0.01. ठीक है आप बना सकते हैं उसे एक अन्य

184
00:08:51,670 --> 00:08:54,670
पेरमिटर लर्निंग अल्गोरिद्म का और

185
00:08:53,380 --> 00:08:58,480
कुछ लोगों का कहना है कि वह काम करता है 
और भी बेहतर

186
00:08:54,670 --> 00:08:59,649
लेकिन मैं शायद ही लोगों को देखता हूँ 
ऐसा करते हुए तो लेकिन

187
00:08:58,480 --> 00:09:01,360
यदि आपको लगता है कोशिश करना चाहिए 
इसे आपकी

188
00:08:59,649 --> 00:09:03,430
ऐप्लिकेशन में आप जानते है कृपया बेझिझक करें

189
00:09:01,360 --> 00:09:05,800
वैसा और आप तब देख सकते हैं कैसे यह

190
00:09:03,430 --> 00:09:08,290
काम करता हैं और कितना अच्छा काम करता है 
और बने रहें साथ

191
00:09:05,800 --> 00:09:09,880
इसके यदि यह देता है अच्छे परिणाम. 
तो मुझे उम्मीद है

192
00:09:08,290 --> 00:09:11,620
कि वह देता है आपको एक समझ कुछ

193
00:09:09,880 --> 00:09:13,870
विकल्पों की ऐक्टिवेशन फ़ंक्शन्स के 
जो आप कर सकते हैं

194
00:09:11,620 --> 00:09:15,940
इस्तेमाल आपके नेटवर्क में. एक विषय

195
00:09:13,870 --> 00:09:18,130
हम देखेंगे डीप लर्निंग में है कि आपके

196
00:09:15,940 --> 00:09:20,110
पास अक्सर हैं बहुत से विभिन्न विकल्प

197
00:09:18,130 --> 00:09:22,089
कैसे आप बनाते हैं आपका न्यूरल नेटवर्क

198
00:09:20,110 --> 00:09:24,430
हिडन यूनिट्स की संख्या से लेकर

199
00:09:22,089 --> 00:09:25,839
ऐक्टिवेशन फ़ंक्शन के चुनाव तक कि कैसे आप

200
00:09:24,430 --> 00:09:28,480
लेते हैं प्रारम्भिक वेट्स जो हम देखेंगे

201
00:09:25,839 --> 00:09:30,880
बाद में. बहुत से विकल्प उस तरह के और ऐसा

202
00:09:28,480 --> 00:09:33,279
होता है कि कभी-कभी कठिन होता है

203
00:09:30,880 --> 00:09:35,649
पाना अच्छे दिशा निर्देश ठीक उसी के लिए जो

204
00:09:33,279 --> 00:09:37,270
करेंगे श्रेष्ठतम कार्य आपकी समस्या के लिए. तो इसलिए

205
00:09:35,649 --> 00:09:39,070
इन कोर्सेज़ के माध्यम से मैं देता रहूँगा

206
00:09:37,270 --> 00:09:40,839
आपको एक समझ उसकी जो मैं देखता हूँ

207
00:09:39,070 --> 00:09:43,450
इंडस्ट्री में कि क्या कम या अधिक

208
00:09:40,839 --> 00:09:45,520
लोकप्रिय है. लेकिन आपकी ऐप्लिकेशन के लिए

209
00:09:43,450 --> 00:09:46,930
आपकी ऐप्लिकेशन के लक्षणों के लिए यह है

210
00:09:45,520 --> 00:09:49,450
वास्तव में बहुत मुश्किल जानना

211
00:09:46,930 --> 00:09:51,400
शुरुआत में सही रूप से कि क्या काम करेगा 
सबसे अच्छा तो

212
00:09:49,450 --> 00:09:52,930
एक सलाह होगी कि यदि आप नहीं है

213
00:09:51,400 --> 00:09:54,940
सुनिश्चित कि कौन सा इन ऐक्टिवेशन

214
00:09:52,930 --> 00:09:57,700
फ़ंक्शन में काम करेगा सबसे अच्छा 
आप जानते हैं परीक्षण करें उन

215
00:09:54,940 --> 00:10:00,010
सबका और फिर आकलन करें उनका 
एक होल्डआउट

216
00:09:57,700 --> 00:10:02,529
वैलिडेशन सेट पर या एक डिवेलप्मेंट सेट पर

217
00:10:00,010 --> 00:10:04,480
जिसके बारे में हम बात करेंगे बाद में और देखें

218
00:10:02,529 --> 00:10:08,350
कौन सा काम करता है बेहतर और फिर चुने

219
00:10:04,480 --> 00:10:10,180
उसे और मुझे लगता है परीक्षण के द्वारा ये

220
00:10:08,350 --> 00:10:13,510
भिन्न विकल्प आपकी ऐप्लिकेशन के लिए

221
00:10:10,180 --> 00:10:16,240
आपको भविष्य अशुद्धि जांच में बेहतर होगा आपके

222
00:10:13,510 --> 00:10:18,130
न्यूरल नेटवर्क आर्किटेक्चर के मुक़ाबले में

223
00:10:16,240 --> 00:10:20,550
विलक्षणों के हमारी समस्या के भी.

224
00:10:18,130 --> 00:10:23,440
अल्गोरिद्म्स के विकास में बजाय

225
00:10:20,550 --> 00:10:25,630
आप जानते हैं यदि मैं आपको कहता हमेशा

226
00:10:23,440 --> 00:10:27,339
इस्तेमाल करें ReLU ऐक्टिवेशन और 
न इस्तेमाल करें

227
00:10:25,630 --> 00:10:29,440
अन्य कुछ. वह शायद करे या न करे

228
00:10:27,339 --> 00:10:30,790
अप्लाई आपकी समस्या पर जिस पर आप

229
00:10:29,440 --> 00:10:32,410
काम कर रहे हैं आप जानते हैं या

230
00:10:30,790 --> 00:10:36,220
या निकट भविष्य में या आने वाले

231
00:10:32,410 --> 00:10:37,870
समय में. ठीक है तो वह था विकल्प

232
00:10:36,220 --> 00:10:39,310
ऐक्टिवेशन फ़ंक्शन के. आपने देखे

233
00:10:37,870 --> 00:10:41,459
सबसे लोकप्रिय ऐक्टिवेशन फ़ंक्शन्स.

234
00:10:39,310 --> 00:10:44,260
वहां एक अन्य सवाल है जो

235
00:10:41,459 --> 00:10:45,160
कभी-कभी पूछा जाता है कि क्यों आपको

236
00:10:44,260 --> 00:10:46,959
ज़रूरत ही है इस्तेमाल करने की

237
00:10:45,160 --> 00:10:49,240
ऐक्टिवेशन फ़ंक्शन की. क्यों न सिर्फ़

238
00:10:46,959 --> 00:10:49,779
हटा दें उसे. तो चलो बात करते हैं

239
00:10:49,240 --> 00:10:52,240
उसकी

240
00:10:49,779 --> 00:10:54,430
अगले वीडियो में और जहाँ आप देखेंगे क्यों

241
00:10:52,240 --> 00:10:58,259
न्यूरल नेटवर्क को चाहिए किसी तरह का

242
00:10:54,430 --> 00:10:58,259
नॉनलिनियर ऐक्टिवेशन फ़ंक्शन.